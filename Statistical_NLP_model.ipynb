{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical NLP Model\n",
        "\n",
        "This program is a Bigram Probability Model in NLP.\n",
        "It analyzes a text and finds which word most frequently follows each word, along with the probability of that happening.\n",
        "\n",
        "\n",
        "⭐ What This Program Does (Short Summary)\n",
        "\n",
        "It builds a bigram model and calculates, for every word w1,\n",
        "which next word w2 has the highest probability of occurring after it.\n",
        "\n",
        "In simple terms:\n",
        "➡ \"For each word, which word usually comes next?\"\n",
        "\n",
        "Probability formula:\n",
        "      \n",
        "    \n",
        "                            Count(W1,W2)\n",
        "                P(W2|W1) = ________________\n",
        "                            Count(W1)\n",
        "\n",
        "\n",
        "⭐ What Type of Program is This?\n",
        "\n",
        "This program is a:\n",
        "\n",
        "--> Bigram Language Model\n",
        "\n",
        "--> Statistical NLP Model\n",
        "\n",
        "--> Next-word probability calculator\n",
        "\n",
        "Before deep learning, NLP used models like:\n",
        "\n",
        "Unigrams\n",
        "\n",
        "Bigrams\n",
        "\n",
        "Trigrams\n",
        "\n",
        "N-gram models\n",
        "\n",
        "⭐ Uses of This Program\n",
        "\n",
        "Predicting next words\n",
        "\n",
        "Text generation (basic)\n",
        "\n",
        "Understanding word patterns\n",
        "\n",
        "Language modeling\n",
        "\n",
        "Autocomplete systems (early versions)"
      ],
      "metadata": {
        "id": "HW4K6xiuEu5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def highest_probability(text):\n",
        "  words=text.split()#Tokenize text:step(1)\n",
        "\n",
        "  bigram_counts=defaultdict(lambda: defaultdict(int))\n",
        "  first_word_counts=defaultdict(int) # count bigram:step(2)\n",
        "\n",
        "  for i in range(len(words)-1):\n",
        "    w1=words[i]\n",
        "    w2=words[i+1]\n",
        "    bigram_counts[w1][w2]+=1\n",
        "    first_word_counts[w1]+=1\n",
        "\n",
        "#calculate prob and find higher1\n",
        "  result={}\n",
        "  for w1 in bigram_counts:\n",
        "    max_prob=0\n",
        "    best_w2=None\n",
        "    for w2 in bigram_counts[w1]:\n",
        "      prob=bigram_counts[w1][w2] / first_word_counts[w1]\n",
        "      if prob>max_prob:\n",
        "        max_prob=prob\n",
        "        best_w2=w2\n",
        "    result[w1]=(best_w2,max_prob)\n",
        "  return result\n",
        "\n",
        "text=\"Education empowers students to learn and education helps teachers to guide and education creates opportunities to learn and grow in education systems around the world.\"\n",
        "output=highest_probability(text)\n",
        "print(\"Highest prob of word(w2) occuring after another word(w1):\")\n",
        "for w1,(w2,prob) in output.items():\n",
        "  print(f\"After '{w1}' -> '{w2}' with probability {prob:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07nBioEPF_Gy",
        "outputId": "0f544ff8-84b1-4bb1-8be6-b3e7dbc16260"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highest prob of word(w2) occuring after another word(w1):\n",
            "After 'Education' -> 'empowers' with probability 1.00\n",
            "After 'empowers' -> 'students' with probability 1.00\n",
            "After 'students' -> 'to' with probability 1.00\n",
            "After 'to' -> 'learn' with probability 0.67\n",
            "After 'learn' -> 'and' with probability 1.00\n",
            "After 'and' -> 'education' with probability 0.67\n",
            "After 'education' -> 'helps' with probability 0.33\n",
            "After 'helps' -> 'teachers' with probability 1.00\n",
            "After 'teachers' -> 'to' with probability 1.00\n",
            "After 'guide' -> 'and' with probability 1.00\n",
            "After 'creates' -> 'opportunities' with probability 1.00\n",
            "After 'opportunities' -> 'to' with probability 1.00\n",
            "After 'grow' -> 'in' with probability 1.00\n",
            "After 'in' -> 'education' with probability 1.00\n",
            "After 'systems' -> 'around' with probability 1.00\n",
            "After 'around' -> 'the' with probability 1.00\n",
            "After 'the' -> 'world.' with probability 1.00\n"
          ]
        }
      ]
    }
  ]
}